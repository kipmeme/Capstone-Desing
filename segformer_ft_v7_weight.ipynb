{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62e4ffc2-e477-431b-9c30-d68a5bd1a052",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.2.1)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.2)\n",
      "Requirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (1.2.9)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\n",
      "Requirement already satisfied: roboflow in /opt/conda/lib/python3.10/site-packages (1.1.47)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: future>=0.17.1 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (1.0.0)\n",
      "Requirement already satisfied: tensorboard!=2.5.0,>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (2.17.1)\n",
      "Requirement already satisfied: torchmetrics==0.2.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (0.2.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from roboflow) (2024.2.2)\n",
      "Requirement already satisfied: idna==3.7 in /opt/conda/lib/python3.10/site-packages (from roboflow) (3.7)\n",
      "Requirement already satisfied: cycler in /opt/conda/lib/python3.10/site-packages (from roboflow) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from roboflow) (1.4.7)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from roboflow) (3.9.2)\n",
      "Requirement already satisfied: opencv-python-headless==4.10.0.84 in /opt/conda/lib/python3.10/site-packages (from roboflow) (4.10.0.84)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in /opt/conda/lib/python3.10/site-packages (from roboflow) (10.2.0)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from roboflow) (2.9.0.post0)\n",
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.10/site-packages (from roboflow) (1.0.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from roboflow) (1.16.0)\n",
      "Requirement already satisfied: urllib3>=1.26.6 in /opt/conda/lib/python3.10/site-packages (from roboflow) (2.1.0)\n",
      "Requirement already satisfied: requests-toolbelt in /opt/conda/lib/python3.10/site-packages (from roboflow) (1.0.0)\n",
      "Requirement already satisfied: filetype in /opt/conda/lib/python3.10/site-packages (from roboflow) (1.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.15.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.66.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.7)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (4.25.5)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (68.2.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->roboflow) (1.3.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->roboflow) (4.54.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->roboflow) (3.1.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch transformers pytorch-lightning datasets roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f55810e7-fdcb-4f20-9f25-b9d5e002201e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.3)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.25.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (23.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.10.10)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.15.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.0.0->evaluate) (0.2.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59a6c28b-c2f1-43ba-a5e1-34aa09f31248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 03:07:15.924831: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-14 03:07:15.928792: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-14 03:07:15.937105: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-14 03:07:15.950390: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-14 03:07:15.954380: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-14 03:07:15.964755: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-14 03:07:16.716668: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f672425d-0977-4e69-9bba-b813a27d1c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    }
   ],
   "source": [
    "import roboflow\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"rWhzxDa8oLFF7F6zw3cG\")\n",
    "project = rf.workspace(\"cd-pq7yy\").project(\"building-defects-xpjmz\")\n",
    "version = project.version(2)\n",
    "dataset = version.download(\"png-mask-semantic\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "236e5418-52aa-4713-bcc0-320339897cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSegmentationDataset(Dataset):\n",
    "    \"\"\"Image (semantic) segmentation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, feature_extractor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Root directory of the dataset containing the images + annotations.\n",
    "            feature_extractor (SegFormerFeatureExtractor): feature extractor to prepare images + segmentation maps.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.feature_extractor = feature_extractor\n",
    "        \n",
    "        # Load class mapping from CSV file (e.g., _classes.csv)\n",
    "        self.classes_csv_file = os.path.join(self.root_dir, \"_classes.csv\")\n",
    "        with open(self.classes_csv_file, 'r') as fid:\n",
    "            data = [l.split(',') for i, l in enumerate(fid) if i != 0]\n",
    "        self.id2label = {x[0]: x[1] for x in data}\n",
    "        \n",
    "        image_file_names = [f for f in os.listdir(self.root_dir) if '.jpg' in f]\n",
    "        mask_file_names = [f for f in os.listdir(self.root_dir) if '.png' in f]\n",
    "        \n",
    "        self.images = sorted(image_file_names)\n",
    "        self.masks = sorted(mask_file_names)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(os.path.join(self.root_dir, self.images[idx]))\n",
    "        segmentation_map = Image.open(os.path.join(self.root_dir, self.masks[idx]))\n",
    "\n",
    "        # Convert segmentation map to numpy array (without ignoring any labels)\n",
    "        segmentation_map = np.array(segmentation_map)\n",
    "\n",
    "        # Apply feature extractor to both image and segmentation map\n",
    "        encoded_inputs = self.feature_extractor(image, segmentation_map, return_tensors=\"pt\")\n",
    "\n",
    "        # Remove batch dimension\n",
    "        for k, v in encoded_inputs.items():\n",
    "            encoded_inputs[k].squeeze_()\n",
    "\n",
    "        return encoded_inputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5be9469-e09d-4b82-87e2-dda5e022b663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SegformerFinetuner(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, id2label, train_dataloader=None, val_dataloader=None, test_dataloader=None, metrics_interval=100):\n",
    "        super(SegformerFinetuner, self).__init__()\n",
    "        self.id2label = id2label\n",
    "        self.metrics_interval = metrics_interval\n",
    "        self.train_dl = train_dataloader\n",
    "        self.val_dl = val_dataloader\n",
    "        self.test_dl = test_dataloader\n",
    "        \n",
    "        self.num_classes = len(id2label.keys())\n",
    "        self.label2id = {v:k for k,v in self.id2label.items()}\n",
    "        \n",
    "        # SegFormer 모델 로드\n",
    "        self.model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "            \"nvidia/segformer-b5-finetuned-cityscapes-1024-1024\", \n",
    "            return_dict=False, \n",
    "            num_labels=self.num_classes,\n",
    "            id2label=self.id2label,\n",
    "            label2id=self.label2id,\n",
    "            ignore_mismatched_sizes=True,\n",
    "        )\n",
    "        \n",
    "        # 클래스 가중치 (4번 클래스에 높은 가중치 설정)\n",
    "        self.class_weights = torch.ones(self.num_classes)  # 기본 가중치는 모두 1\n",
    "        self.class_weights[4] = 15.0  # 4번 클래스에 가중치 10을 적용\n",
    "        \n",
    "        self.train_mean_iou = load_metric(\"mean_iou\")\n",
    "        self.val_mean_iou = load_metric(\"mean_iou\")\n",
    "        self.test_mean_iou = load_metric(\"mean_iou\")\n",
    "        \n",
    "    def forward(self, images, masks):\n",
    "        outputs = self.model(pixel_values=images, labels=masks)\n",
    "        return outputs\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        images, masks = batch['pixel_values'], batch['labels']\n",
    "        outputs = self(images, masks)\n",
    "        loss, logits = outputs[0], outputs[1]\n",
    "        \n",
    "        # Logits 업샘플링하여 원본 크기와 맞추기\n",
    "        upsampled_logits = nn.functional.interpolate(\n",
    "            logits, \n",
    "            size=masks.shape[-2:], \n",
    "            mode=\"bilinear\", \n",
    "            align_corners=False\n",
    "        )\n",
    "\n",
    "        predicted = upsampled_logits.argmax(dim=1)\n",
    "\n",
    "        # 클래스 가중치를 적용한 cross-entropy 손실 계산\n",
    "        loss = F.cross_entropy(upsampled_logits, masks, weight=self.class_weights.to(self.device))\n",
    "\n",
    "        # IoU 계산\n",
    "        self.train_mean_iou.add_batch(\n",
    "            predictions=predicted.detach().cpu().numpy(), \n",
    "            references=masks.detach().cpu().numpy()\n",
    "        )\n",
    "        \n",
    "        if batch_nb % self.metrics_interval == 0:\n",
    "            metrics = self.train_mean_iou.compute(\n",
    "                num_labels=self.num_classes, \n",
    "                ignore_index=255, \n",
    "                reduce_labels=False,\n",
    "            )\n",
    "            metrics = {'loss': loss, \"mean_iou\": metrics[\"mean_iou\"], \"mean_accuracy\": metrics[\"mean_accuracy\"]}\n",
    "            \n",
    "            for k,v in metrics.items():\n",
    "                self.log(k, v)\n",
    "            \n",
    "            return metrics\n",
    "        else:\n",
    "            return {'loss': loss}\n",
    "    \n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        images, masks = batch['pixel_values'], batch['labels']\n",
    "        outputs = self(images, masks)\n",
    "        loss, logits = outputs[0], outputs[1]\n",
    "        \n",
    "        upsampled_logits = nn.functional.interpolate(\n",
    "            logits, \n",
    "            size=masks.shape[-2:], \n",
    "            mode=\"bilinear\", \n",
    "            align_corners=False\n",
    "        )\n",
    "        \n",
    "        predicted = upsampled_logits.argmax(dim=1)\n",
    "\n",
    "        # Validation에서도 가중치를 적용한 손실 계산\n",
    "        loss = F.cross_entropy(upsampled_logits, masks, weight=self.class_weights.to(self.device))\n",
    "\n",
    "        # IoU 계산\n",
    "        self.val_mean_iou.add_batch(\n",
    "            predictions=predicted.detach().cpu().numpy(), \n",
    "            references=masks.detach().cpu().numpy()\n",
    "        )\n",
    "        \n",
    "        return {'val_loss': loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        metrics = self.val_mean_iou.compute(\n",
    "              num_labels=self.num_classes, \n",
    "              ignore_index=255, \n",
    "              reduce_labels=False,\n",
    "          )\n",
    "        \n",
    "        avg_val_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        val_mean_iou = metrics[\"mean_iou\"]\n",
    "        val_mean_accuracy = metrics[\"mean_accuracy\"]\n",
    "        \n",
    "        metrics = {\"val_loss\": avg_val_loss, \"val_mean_iou\": val_mean_iou, \"val_mean_accuracy\": val_mean_accuracy}\n",
    "        for k,v in metrics.items():\n",
    "            self.log(k, v)\n",
    "\n",
    "        return metrics\n",
    "    \n",
    "    def test_step(self, batch, batch_nb):\n",
    "        images, masks = batch['pixel_values'], batch['labels']\n",
    "        outputs = self(images, masks)\n",
    "        loss, logits = outputs[0], outputs[1]\n",
    "        \n",
    "        upsampled_logits = nn.functional.interpolate(\n",
    "            logits, \n",
    "            size=masks.shape[-2:], \n",
    "            mode=\"bilinear\", \n",
    "            align_corners=False\n",
    "        )\n",
    "        \n",
    "        predicted = upsampled_logits.argmax(dim=1)\n",
    "\n",
    "        # Test에서도 가중치를 적용한 손실 계산\n",
    "        loss = F.cross_entropy(upsampled_logits, masks, weight=self.class_weights.to(self.device))\n",
    "\n",
    "        # IoU 계산\n",
    "        self.test_mean_iou.add_batch(\n",
    "            predictions=predicted.detach().cpu().numpy(), \n",
    "            references=masks.detach().cpu().numpy()\n",
    "        )\n",
    "            \n",
    "        return {'test_loss': loss}\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        metrics = self.test_mean_iou.compute(\n",
    "              num_labels=self.num_classes, \n",
    "              ignore_index=255, \n",
    "              reduce_labels=False,\n",
    "          )\n",
    "       \n",
    "        avg_test_loss = torch.stack([x[\"test_loss\"] for x in outputs]).mean()\n",
    "        test_mean_iou = metrics[\"mean_iou\"]\n",
    "        test_mean_accuracy = metrics[\"mean_accuracy\"]\n",
    "\n",
    "        metrics = {\"test_loss\": avg_test_loss, \"test_mean_iou\": test_mean_iou, \"test_mean_accuracy\": test_mean_accuracy}\n",
    "        for k,v in metrics.items():\n",
    "            self.log(k,v)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam([p for p in self.parameters() if p.requires_grad], lr=2e-05, eps=1e-08)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.train_dl\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return self.val_dl\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return self.test_dl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "197de299-4be0-4ded-8cb1-656cd18ab2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "# IoU 계산 함수\n",
    "def compute_iou(predictions, targets, num_classes):\n",
    "    ious = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = predictions == cls\n",
    "        target_inds = targets == cls\n",
    "        intersection = torch.sum(pred_inds & target_inds)\n",
    "        union = torch.sum(pred_inds | target_inds)\n",
    "        if union == 0:\n",
    "            ious.append(float('nan'))  # NaN 대신 기본값을 설정 가능\n",
    "        else:\n",
    "            ious.append(float(intersection) / float(union))\n",
    "    return torch.tensor(ious).nanmean()  # NaN 제외 평균 IoU\n",
    "\n",
    "# SegFormer fine-tuning 클래스\n",
    "class SegformerFinetuner(pl.LightningModule):\n",
    "    def __init__(self, id2label, train_dataloader=None, val_dataloader=None, test_dataloader=None, metrics_interval=100):\n",
    "        super(SegformerFinetuner, self).__init__()\n",
    "        self.id2label = id2label\n",
    "        self.metrics_interval = metrics_interval\n",
    "        self.train_dl = train_dataloader\n",
    "        self.val_dl = val_dataloader\n",
    "        self.test_dl = test_dataloader\n",
    "        \n",
    "        self.num_classes = len(id2label.keys())\n",
    "        self.label2id = {v: k for k, v in self.id2label.items()}\n",
    "        \n",
    "        # Pretrained SegFormer 모델 로드\n",
    "        self.model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "            \"nvidia/segformer-b5-finetuned-cityscapes-1024-1024\",\n",
    "            num_labels=self.num_classes,\n",
    "            id2label=self.id2label,\n",
    "            label2id=self.label2id,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        \n",
    "        # 클래스 가중치 (4번 클래스에 가중치 부여)\n",
    "        self.class_weights = torch.ones(self.num_classes)\n",
    "        self.class_weights[4] = 15.0  # 클래스 4에 높은 가중치 부여\n",
    "\n",
    "    def forward(self, images, masks):\n",
    "        outputs = self.model(pixel_values=images, labels=masks)\n",
    "        return outputs\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        images, masks = batch['pixel_values'], batch['labels']\n",
    "        \n",
    "        outputs = self(images, masks)\n",
    "        loss, logits = outputs[0], outputs[1]\n",
    "\n",
    "        # Logits upsampling to match mask size\n",
    "        upsampled_logits = torch.nn.functional.interpolate(\n",
    "            logits, \n",
    "            size=masks.shape[-2:], \n",
    "            mode=\"bilinear\", \n",
    "            align_corners=False\n",
    "        )\n",
    "        \n",
    "        predicted = upsampled_logits.argmax(dim=1)\n",
    "\n",
    "        # 가중치를 적용한 cross-entropy 손실 계산\n",
    "        loss = F.cross_entropy(upsampled_logits, masks, weight=self.class_weights.to(self.device))\n",
    "        \n",
    "        # IoU 계산\n",
    "        mean_iou = compute_iou(predicted, masks, self.num_classes)\n",
    "        \n",
    "        metrics = {'loss': loss, 'mean_iou': mean_iou}\n",
    "        \n",
    "        if batch_nb % self.metrics_interval == 0:\n",
    "            for k, v in metrics.items():\n",
    "                self.log(k, v)\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        images, masks = batch['pixel_values'], batch['labels']\n",
    "        \n",
    "        outputs = self(images, masks)\n",
    "        loss, logits = outputs[0], outputs[1]\n",
    "\n",
    "        # Logits upsampling to match mask size\n",
    "        upsampled_logits = torch.nn.functional.interpolate(\n",
    "            logits, \n",
    "            size=masks.shape[-2:], \n",
    "            mode=\"bilinear\", \n",
    "            align_corners=False\n",
    "        )\n",
    "        \n",
    "        predicted = upsampled_logits.argmax(dim=1)\n",
    "        \n",
    "        # Validation에서도 가중치 적용\n",
    "        loss = F.cross_entropy(upsampled_logits, masks, weight=self.class_weights.to(self.device))\n",
    "        \n",
    "        # IoU 계산\n",
    "        mean_iou = compute_iou(predicted, masks, self.num_classes)\n",
    "        \n",
    "        return {'val_loss': loss, 'val_mean_iou': mean_iou}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_val_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        avg_val_mean_iou = torch.stack([x[\"val_mean_iou\"] for x in outputs]).mean()\n",
    "\n",
    "        metrics = {\"val_loss\": avg_val_loss, \"val_mean_iou\": avg_val_mean_iou}\n",
    "        for k, v in metrics.items():\n",
    "            self.log(k, v)\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def test_step(self, batch, batch_nb):\n",
    "        images, masks = batch['pixel_values'], batch['labels']\n",
    "        \n",
    "        outputs = self(images, masks)\n",
    "        loss, logits = outputs[0], outputs[1]\n",
    "\n",
    "        # Logits upsampling to match mask size\n",
    "        upsampled_logits = torch.nn.functional.interpolate(\n",
    "            logits, \n",
    "            size=masks.shape[-2:], \n",
    "            mode=\"bilinear\", \n",
    "            align_corners=False\n",
    "        )\n",
    "        \n",
    "        predicted = upsampled_logits.argmax(dim=1)\n",
    "\n",
    "        # Test에서도 가중치 적용\n",
    "        loss = F.cross_entropy(upsampled_logits, masks, weight=self.class_weights.to(self.device))\n",
    "        \n",
    "        # IoU 계산\n",
    "        mean_iou = compute_iou(predicted, masks, self.num_classes)\n",
    "        \n",
    "        return {'test_loss': loss, 'test_mean_iou': mean_iou}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_test_loss = torch.stack([x[\"test_loss\"] for x in outputs]).mean()\n",
    "        avg_test_mean_iou = torch.stack([x[\"test_mean_iou\"] for x in outputs]).mean()\n",
    "\n",
    "        metrics = {\"test_loss\": avg_test_loss, \"test_mean_iou\": avg_test_mean_iou}\n",
    "        for k, v in metrics.items():\n",
    "            self.log(k, v)\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam([p for p in self.parameters() if p.requires_grad], lr=2e-05, eps=1e-08)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_dl\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.val_dl\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.test_dl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdd9e8c9-9671-456c-8495-99594cb7e1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The class SegformerFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use SegformerImageProcessor instead.\n",
      "The following named arguments are not valid for `SegformerFeatureExtractor.__init__` and were ignored: 'feature_extractor_type'\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b5-finetuned-cityscapes-1024-1024 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.weight: found shape torch.Size([19, 768, 1, 1]) in the checkpoint and torch.Size([5, 768, 1, 1]) in the model instantiated\n",
      "- decode_head.classifier.bias: found shape torch.Size([19]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = SegformerFeatureExtractor.from_pretrained(\"nvidia/segformer-b5-finetuned-cityscapes-1024-1024\")\n",
    "feature_extractor.do_reduce_labels = False\n",
    "feature_extractor.size = 512\n",
    "\n",
    "train_dataset = SemanticSegmentationDataset(f\"{dataset.location}/train/\", feature_extractor)\n",
    "val_dataset = SemanticSegmentationDataset(f\"{dataset.location}/valid/\", feature_extractor)\n",
    "test_dataset = SemanticSegmentationDataset(f\"{dataset.location}/test/\", feature_extractor)\n",
    "\n",
    "batch_size = 4\n",
    "num_workers = 0\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=0)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "segformer_finetuner = SegformerFinetuner(\n",
    "    train_dataset.id2label, \n",
    "    train_dataloader=train_dataloader, \n",
    "    val_dataloader=val_dataloader, \n",
    "    test_dataloader=test_dataloader, \n",
    "    metrics_interval=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d1021a-ad1b-451f-a5af-83f746ebbf87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    }
   ],
   "source": [
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\", \n",
    "    min_delta=0.00, \n",
    "    patience=, \n",
    "    verbose=False, \n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(save_top_k=1, monitor=\"val_loss\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=[1], \n",
    "    callbacks=[early_stop_callback, checkpoint_callback],\n",
    "    max_epochs=500,\n",
    "    val_check_interval=len(train_dataloader),\n",
    ")\n",
    "trainer.fit(segformer_finetuner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c051395-d756-459c-b455-9248d42d3436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average IoU: 0.08172360687864229\n",
      "Class 0: Precision=0.4699793903165759, Recall=0.18847458954341678, F1-score=0.2689630662652155\n",
      "Class 1: Precision=0.18301535205015532, Recall=0.1141222009772243, F1-score=0.14048916045689816\n",
      "Class 2: Precision=0.014977487904955343, Recall=0.12255718092009951, F1-score=0.026683486146758906\n",
      "Overall Accuracy: 0.1688995361328125\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# IoU 계산 함수\n",
    "def compute_iou(predictions, targets, num_classes):\n",
    "    ious = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = predictions == cls\n",
    "        target_inds = targets == cls\n",
    "        intersection = np.sum(pred_inds & target_inds)\n",
    "        union = np.sum(pred_inds | target_inds)\n",
    "        if union == 0:\n",
    "            ious.append(float('nan'))  # NaN 대신 처리할 값을 넣을 수 있음\n",
    "        else:\n",
    "            ious.append(float(intersection) / float(union))\n",
    "    return np.nanmean(ious)  # NaN을 제외한 평균 IoU\n",
    "\n",
    "def compute_bbox(predictions, targets, num_classes):\n",
    "    bbox = []\n",
    "    for cls in range(num_classegs):\n",
    "        pred_inds = predictions == cls\n",
    "        target_inds = targets == cls\n",
    "        intersection = np.sum\n",
    "\n",
    "# 클래스별로 정밀도, 재현율, F1-score, 정확도 계산 함수\n",
    "def compute_classwise_metrics(predictions, targets, num_classes):\n",
    "    precision = precision_score(targets.flatten(), predictions.flatten(), average=None, labels=range(num_classes))\n",
    "    recall = recall_score(targets.flatten(), predictions.flatten(), average=None, labels=range(num_classes))\n",
    "    f1 = f1_score(targets.flatten(), predictions.flatten(), average=None, labels=range(num_classes))\n",
    "    accuracy = accuracy_score(targets.flatten(), predictions.flatten())\n",
    "    \n",
    "    return precision, recall, f1, accuracy\n",
    "\n",
    "# 모델 추론 및 성능 지표 계산 함수\n",
    "def evaluate_model_on_test_data(dataloader, model, num_classes):\n",
    "    iou_list = []\n",
    "    precision_list, recall_list, f1_list = [], [], []\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        images, masks = batch['pixel_values'], batch['labels']\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(pixel_values=images)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        upsampled_logits = torch.nn.functional.interpolate(\n",
    "            logits, \n",
    "            size=masks.shape[-2:], \n",
    "            mode=\"bilinear\", \n",
    "            align_corners=False\n",
    "        )\n",
    "        \n",
    "        predicted_mask = upsampled_logits.argmax(dim=1).cpu().numpy()\n",
    "        masks = masks.cpu().numpy()\n",
    "\n",
    "        # IoU 계산\n",
    "        iou = compute_iou(predicted_mask, masks, num_classes)\n",
    "        iou_list.append(iou)\n",
    "\n",
    "        # 정밀도, 재현율, F1-score 계산\n",
    "        precision, recall, f1, accuracy = compute_classwise_metrics(predicted_mask, masks, num_classes)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_list.append(f1)\n",
    "\n",
    "    avg_iou = np.mean(iou_list)\n",
    "    avg_precision = np.mean(precision_list, axis=0)\n",
    "    avg_recall = np.mean(recall_list, axis=0)\n",
    "    avg_f1 = np.mean(f1_list, axis=0)\n",
    "\n",
    "    return avg_iou, avg_precision, avg_recall, avg_f1, accuracy\n",
    "\n",
    "# 모델 성능 평가\n",
    "num_classes = 3  # 클래스 개수\n",
    "avg_iou, avg_precision, avg_recall, avg_f1, accuracy = evaluate_model_on_test_data(test_dataloader, segformer_finetuner.model, num_classes)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"Average IoU: {avg_iou}\")\n",
    "for cls in range(num_classes):\n",
    "    print(f\"Class {cls}: Precision={avg_precision[cls]}, Recall={avg_recall[cls]}, F1-score={avg_f1[cls]}\")\n",
    "print(f\"Overall Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3d0f5ab-d502-4ee9-bd37-555eb624d13e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(ious)\u001b[38;5;241m.\u001b[39mnanmean()  \u001b[38;5;66;03m# NaN 제외 평균 Boundary IoU\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# SegformerFinetuner 클래스에 성능 평가 메서드 추가\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSegformerFinetuner\u001b[39;00m(\u001b[43mpl\u001b[49m\u001b[38;5;241m.\u001b[39mLightningModule):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# 기존 코드 유지...\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_metrics\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataloader):\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# 모델을 평가 모드로 전환\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pl' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from scipy.ndimage import binary_erosion\n",
    "# Dice Coefficient 계산 함수\n",
    "def compute_dice_coefficient(predictions, targets, num_classes):\n",
    "    dice_scores = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = predictions == cls\n",
    "        target_inds = targets == cls\n",
    "        intersection = torch.sum(pred_inds & target_inds)\n",
    "        total = torch.sum(pred_inds) + torch.sum(target_inds)\n",
    "        if total == 0:\n",
    "            dice_scores.append(float('nan'))  # NaN 대신 기본값을 설정 가능\n",
    "        else:\n",
    "            dice_scores.append((2.0 * float(intersection)) / float(total))\n",
    "    return torch.tensor(dice_scores).nanmean()  # NaN 제외 평균 Dice Coefficient\n",
    "# Boundary IoU 계산 함수\n",
    "def compute_boundary_iou(predictions, targets, num_classes, boundary_width=1):\n",
    "    ious = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = predictions == cls\n",
    "        target_inds = targets == cls\n",
    "        # 경계 추출: erosion을 이용하여 경계 영역을 얻음\n",
    "        pred_boundary = pred_inds ^ binary_erosion(pred_inds, structure=np.ones((boundary_width, boundary_width)))\n",
    "        target_boundary = target_inds ^ binary_erosion(target_inds, structure=np.ones((boundary_width, boundary_width)))\n",
    "        intersection = torch.sum(pred_boundary & target_boundary)\n",
    "        union = torch.sum(pred_boundary | target_boundary)\n",
    "        if union == 0:\n",
    "            ious.append(float('nan'))  # NaN 대신 기본값을 설정 가능\n",
    "        else:\n",
    "            ious.append(float(intersection) / float(union))\n",
    "    return torch.tensor(ious).nanmean()  # NaN 제외 평균 Boundary IoU\n",
    "# SegformerFinetuner 클래스에 성능 평가 메서드 추가\n",
    "class SegformerFinetuner(pl.LightningModule):\n",
    "    # 기존 코드 유지...\n",
    "    def evaluate_metrics(self, dataloader):\n",
    "        self.model.eval()  # 모델을 평가 모드로 전환\n",
    "        dice_scores = []\n",
    "        boundary_ious = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                images, masks = batch['pixel_values'], batch['labels']\n",
    "                outputs = self(images, masks)\n",
    "                logits = outputs[1]\n",
    "                # Logits upsampling to match mask size\n",
    "                upsampled_logits = torch.nn.functional.interpolate(\n",
    "                    logits,\n",
    "                    size=masks.shape[-2:],\n",
    "                    mode=\"bilinear\",\n",
    "                    align_corners=False\n",
    "                )\n",
    "                predicted = upsampled_logits.argmax(dim=1)\n",
    "                # Dice Coefficient 및 Boundary IoU 계산\n",
    "                dice_score = compute_dice_coefficient(predicted, masks, self.num_classes)\n",
    "                boundary_iou = compute_boundary_iou(predicted, masks, self.num_classes)\n",
    "                dice_scores.append(dice_score)\n",
    "                boundary_ious.append(boundary_iou)\n",
    "        avg_dice = torch.stack(dice_scores).mean()\n",
    "        avg_boundary_iou = torch.stack(boundary_ious).mean()\n",
    "        print(f\"Average Dice Coefficient: {avg_dice:.4f}\")\n",
    "        print(f\"Average Boundary IoU: {avg_boundary_iou:.4f}\")\n",
    "        return {'avg_dice': avg_dice, 'avg_boundary_iou': avg_boundary_iou}\n",
    "# 학습된 모델에 대해 테스트 데이터셋을 사용하여 평가 수행\n",
    "evaluator = segformer_finetuner.evaluate_metrics(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5d0212a7-1aab-40a6-b0fc-f313a4a7f1fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_18.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_95.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_69.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_98.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_56.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_58.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_11.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_87.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_20.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_14.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_66.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_13.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_47.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_96.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_90.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_30.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_92.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_71.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_19.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_93.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_15.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_48.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_22.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_57.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_40.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_49.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_43.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_65.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_99.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_77.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_44.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_84.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_45.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_86.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_38.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_21.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_73.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_4.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_35.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_82.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_75.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_23.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_7.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_80.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_54.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_79.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_60.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_32.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_31.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_6.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_8.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_9.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_16.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_25.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_3.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_29.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_28.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_85.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_94.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_83.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_17.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_34.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_41.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_37.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_5.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_24.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_76.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_62.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_78.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_12.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_50.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_100.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_53.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_36.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_33.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_70.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_39.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_1.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_81.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_42.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_64.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_67.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_88.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_89.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_52.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_91.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_26.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_63.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_72.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_97.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_59.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_27.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_68.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_46.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_74.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_61.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_10.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_55.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_2.png\n",
      "Saved segmented image with overlay: ft_v7_model_infer_result/segmented_51.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Color map 설정 (배경, 건물, 크랙 등)\n",
    "color_map = {\n",
    "    0: (0, 0, 0),      # 배경\n",
    "    1: (255, 255, 255),  # 건물\n",
    "    2: (255, 0, 0),     # 크랙\n",
    "}\n",
    "\n",
    "# 예측된 마스크를 시각화하는 함수\n",
    "def prediction_to_vis(prediction):\n",
    "    vis_shape = prediction.shape + (3,)\n",
    "    vis = np.zeros(vis_shape)\n",
    "    for i, c in color_map.items():\n",
    "        vis[prediction == i] = color_map[i]\n",
    "    return Image.fromarray(vis.astype(np.uint8))\n",
    "\n",
    "# 이미지 전처리 함수\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "    return inputs, image\n",
    "\n",
    "# 폴더 내 모든 이미지에 대해 inference를 수행하고 결과를 시각화하는 함수\n",
    "def run_inference_on_folder(folder_path, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)  # 결과를 저장할 폴더 생성\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(('.png', '.jpg', '.jpeg')):  # 이미지 파일만 처리\n",
    "            image_path = os.path.join(folder_path, file_name)\n",
    "            inputs, input_image = preprocess_image(image_path)\n",
    "\n",
    "            # 모델 추론\n",
    "            with torch.no_grad():\n",
    "                outputs = segformer_finetuner.model(pixel_values=inputs['pixel_values'])\n",
    "\n",
    "            logits = outputs.logits\n",
    "            upsampled_logits = torch.nn.functional.interpolate(\n",
    "                logits,\n",
    "                size=(input_image.size[1], input_image.size[0]),\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=False\n",
    "            )\n",
    "\n",
    "            # 각 픽셀에서 가장 가능성 높은 클래스를 선택\n",
    "            predicted_mask = upsampled_logits.argmax(dim=1).cpu().numpy()[0]\n",
    "\n",
    "            # 시각화\n",
    "            mask = prediction_to_vis(predicted_mask)\n",
    "            mask = mask.resize(input_image.size)\n",
    "            mask = mask.convert(\"RGBA\")\n",
    "\n",
    "            # 원본 이미지와 예측된 마스크를 오버레이\n",
    "            input_image = input_image.convert(\"RGBA\")\n",
    "            overlay_img = Image.blend(input_image, mask, 0.5)\n",
    "\n",
    "            # 결과 이미지 저장\n",
    "            output_path = os.path.join(output_folder, f\"segmented_{file_name}\")\n",
    "            overlay_img.save(output_path)\n",
    "            print(f\"Saved segmented image with overlay: {output_path}\")\n",
    "\n",
    "# 사용 예시\n",
    "input_folder = \"sv_data\"  # 추론할 이미지가 있는 폴더 경로\n",
    "output_folder = \"ft_v7_model_infer_result\"  # 결과를 저장할 폴더 경로\n",
    "run_inference_on_folder(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3771ae7-5485-417e-b702-9c8fe2e570f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291c9f81-63b5-465d-b6e9-2e432dac3c29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
